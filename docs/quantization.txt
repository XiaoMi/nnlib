
/*
 * Copyright (c) 2016-2019, The Linux Foundation. All rights reserved.
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted (subject to the limitations in the
 * disclaimer below) provided that the following conditions are met:
 *
 *    * Redistributions of source code must retain the above copyright
 *      notice, this list of conditions and the following disclaimer.
 *
 *    * Redistributions in binary form must reproduce the above
 *      copyright notice, this list of conditions and the following
 *      disclaimer in the documentation and/or other materials provided
 *      with the distribution.
 *
 *    * Neither the name of The Linux Foundation nor the names of its
 *      contributors may be used to endorse or promote products derived
 *      from this software without specific prior written permission.
 *
 * NO EXPRESS OR IMPLIED LICENSES TO ANY PARTY'S PATENT RIGHTS ARE
 * GRANTED BY THIS LICENSE. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT
 * HOLDERS AND CONTRIBUTORS "AS IS" AND ANY EXPRESS OR IMPLIED
 * WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF
 * MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.
 * IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR
 * ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
 * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE
 * GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
 * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER
 * IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR
 * OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN
 * IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 *
 */

All about quantization schemes



Instead of having a simple fixed point, or a shared exponent, we have an
unusual quantization scheme.

We have integer values, typically unsigned 8 bits, that represent 256 steps
between two endpoints (inclusive).  Instead of just the integral values, we
have the integral values plus the floating point value that "0" corresponds to,
plus the floating point value that "255" corresponds to.

Another way to look at this, is that we (approximately) have the offset of 0.0
in the number space, and the size of each integer step.  (A-a_offset)*stepsize
can then be used to convert the u8 value "A" into its floating point value.

The interesting thing about this is that the same number system can be used to 
encode signed or unsigned or even asymmetric number ranges well.

The bad thing about it is that adding and multiplying is more complicated.

For adding we need to first convert the numbers to a common number space before
adding, then shrink them back down.

For multiplying, we need to compute (A-a_offset)*(B-b_offset).  Since we are
often doing dot products where a lot of the values are constant, we can compute
sum(B) and multiply a_offset*sumb(B) more easily.  Similarly, we can multiply
b_offset*a_offset*length(filter) to handle that constant*constant case.  The
b_offset*sum(A) can be very tricky, but for filtering integral image style 
techniques can be effective.

We always want to try and keep 0 in the range, and make 0 an integral value.

The current code passes around min/max values.  Android NN is using stepsize /
offset values.  I think it would be cleaner if we migrate to that over time,
although we're clearly using it internally already.

https://petewarden.com/2017/06/22/what-ive-learned-about-neural-network-quantization/



===================================================
Quantized formats
===================================================
In all cases, a value is dequantized by the formula
	y = (quantized_val - zero_code) * stepsize

---------------
qint32
---------------
	element   :  int32
	zero_code :  fixed at 0
	minval    :  stepsize * -(2^31)
	maxval    :  stepsize * (2^31)
	Note:
		minval = -maxval
		stepsize can be found as (maxval-minval)/(float)(2^32)
		It is very common for the actual range of data in a
		'qint32' tensor to be far less than the full range as
		    indicated by minval and maxval; this is not
			not the case for the other formats.


---------------
qu8
---------------
	element   :  uint8
	zero_code :  0..255
	minval    :  stepsize * (-zero_code)/255.0
	maxval    :  stepsize * (255-zero_code)/255.0
	Note:
		maxval >=0; minval <=0
		stepsize can be found as (maxval-minval)/255.
		zero_code can be found as -minval/stepsize
		   It should be an integer value, otherwise the downstream
		   node may not correctly interpret the value. In particular
		   maxval = -minval is not a proper range (since zero_code = 127.5).
		   To support a -1.0 to 1.0 range, you can use e.g
		      minval = -1.0, maxval = 1.007874 (i.e. 128/127)

---------------
qint16
---------------
	element   :  int16
	zero_code :  fixed at 0
	minval    :  stepsize * -32768.0
	maxval    :  stepsize * 32768.0
	Note:
		minval = -maxval
		stepsize can be found as (maxval-minval)/65536.
		The maximum *representable* value is actually maxval-stepsize.
		A qint16 tensor can be converted to qu16 exactly, by inverting
		   bit 15 of each value, and using the same minval and maxval.

---------------
qu16
---------------
	element   :  uint16
	zero_code :  0..65535
	minval    :  stepsize * (-zero_code)/65536.0
	maxval    :  stepsize * (65536-zero_code)/65536.0
	Note:
		maxval > 0; minval <=0
		stepsize can be found as (maxval-minval)/65536.
		The maximum *representable* value is actually maxval-stepsize.
		zero_code can be found as -minval/stepsize
		   It should be an integer value, otherwise the downstream
		   node may not correctly interpret the value.
		   However, unlike the qu8 case, minval = -maxval is
		   OK and results in zero_code = 32768.
		Even in the best case, a calculated zero_code may be different
		   from the nearest integer by +/- 0.01 or so, due to rounding errors.
		minval can be zero, but maxval cannot; it should be at
		   least -minval/65535 in order to meet the conditions
		   above.
